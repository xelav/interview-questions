# Data Science Interview Questions

## Statistics

- Матожидание, дисперсия суммы случайных величин и случайной величины с коэффициентом

Матожидание - "среднее арифметическое" функции распределения:

$\mathbb{E} X = \int xf(x)dx$ для непрерывных случайных величин

$\mathbb{E} X = \sum_{i} x_i  p_i$ для дискретных

Матожидание линейно:

$$\mathbb{E} [aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]$$

Дисперсия - мера разброса случайной величины от матожидания:

$$\mathbb{D}[X] = \mathbb{E}[(\mathbb{E}X - X)^2]$$

Или, если воспользоваться свойствами матожидания:

$$\mathbb{D}[X] = \mathbb{E}[X^2] + \mathbb{E}[X]^2$$

Дисперсия не линейна:

$$D[kX] = k^2D[X]$$
$$D[X+Y] = D[X] + D[Y] + cov(X,Y)$$

- ЦПТ, закон больших чисел
- Правило трёх сигм

Вероятность того, что **любая** случайная величина отклонится от своего среднего значения менее чем на $3\sigma$ -

$$P(|\xi - E\xi| < 3\sigma) \ge \bold{\frac{8}{9}}$$

Для нормальных распределений это число составляет приблизительно **0.9973**

- Распределения с тяжелыми хвостами. Примеры, и в чем их проблема?

https://www.keldysh.ru/papers/2003/source/book/gmalin/gl5.htm#:~:text=%D0%A1%D1%83%D1%82%D1%8C%20%D0%B8%D1%85%20%D0%B2%D1%81%D0%B5%D1%85%20%D1%81%D0%BE%D1%81%D1%82%D0%BE%D0%B8%D1%82%20%D0%B2,%D0%BF%D1%80%D0%B5%D0%BD%D0%B5%D0%B1%D1%80%D0%B5%D1%87%D1%8C%20%D0%BA%D1%80%D1%83%D0%BF%D0%BD%D1%8B%D0%BC%D0%B8%2C%20%D0%BD%D0%BE%20%D1%80%D0%B5%D0%B4%D0%BA%D0%B8%D0%BC%D0%B8%20%D1%81%D0%BE%D0%B1%D1%8B%D1%82%D0%B8%D1%8F%D0%BC%D0%B8.&text=%D0%94%D0%BB%D1%8F%20%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F%20%D0%9F%D0%B0%D1%80%D0%B5%D1%82%D0%BE%20%D1%81%20a,%D1%83%D0%B6%D0%B5%20%D1%81%D1%80%D0%B5%D0%B4%D0%BD%D0%B5%D0%B5%20M1%20%3D%20%C2%A5.
https://dyakonov.org/2016/03/04/%D1%82%D1%8F%D0%B6%D1%91%D0%BB%D1%8B%D0%B5-%D1%85%D0%B2%D0%BE%D1%81%D1%82%D1%8B/

Распределения с тяжелыми хвостами - распределения имеющие крупные, но редкие события, которыми нельзя пренебречь.

Пример - распределение доходов людей в мире.

Проблема в том, что эти распределения не имеют конечного матожидания и для них **не работает ЦПТ**.

- Как подсчитать матожидание выборки от распределения с тяжелыми хвостами?
- Какие статистические тесты существуют? Их основные предположения
- Отличие параметрических тестов от непараметрических

https://www.healthknowledge.org.uk/public-health-textbook/research-methods/1b-statistical-methods/parametric-nonparametric-tests#:~:text=Parametric%20tests%20are%20those%20that,used%20for%20non%2DNormal%20variables


- Ошибки первого и второго рода

Это ошибки при бинарной классификации.

Ошибка **I** рода - объект ошибочно относится к **положительному** классу (студент не учил, но сдал экзамен).

Ошибка **II** рода - объект ошибочно отностится к **отрицательному** классу (студент учил, но не сдал экзамен)

- Что такое p-value?
- Предположения линейной модели

Модель линейной регрессии справедлива, если выполняются следующие предположения (**условия Гаусса-Маркова**):

1. **Линейность** - Отношение между X и Y линейное
2. **Нормальность** - X и Y имеют многомерные нормальные распределения
3. **Отсутсвие корреляции** наблюдений
4. Ошибки не носят систематического характера: $E[\epsilon_i] =0$
5. **Гомоскедастичность** - дисперсия ошибок одинакова
6. **Отсутствие автокорреляции** - $\epsilon_i$ распределены независимо от $\epsilon_j$ при различных $i$ и $j$

- Теорема Байеса. Примеры, когда она важна
- R statisic
- If we will add a random noise feature to a linear model will R statistic on train improve? 
- Ковариация. Как связана независимость двух случайных величин и нулеввая ковариация?

Ковариация - мера **линейной** зависимости двух случайных величин

$$cov(X,Y)=E[(X-EX)(Y-EY)]$$

Нулевая ковариация показывает отсутвие **линейной** связи между случайными величинами.

[//]: # (TODO: ссылка на вопрос про X и X^2)

- Корреляция Пирсона

http://www.machinelearning.ru/wiki/index.php?title=%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%BA%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D0%B8_%D0%9F%D0%B8%D1%80%D1%81%D0%BE%D0%BD%D0%B0

Корреляция Пирсона (чаще просто "*корреляция*") являтся нормированной ковариацией:

$$r_{XY}= \frac{cov_{XY}}{\sigma_X\sigma_Y}$$
,где $\sigma_Z = \sqrt{D[Z]}$

Необходимо понимать различие понятий "независимость" и "некоррелированность". Из первого следует второе, но не наоборот.

- Корреляция Спирмена. Преимущества перед корреляцией Пирсона

- V-коэффициент Краммера

- Как подсчитать связь категориальной переменной и непрервыной?
- Чему будет равна ковариция случайных величин X и X^2?
- Дисперсия суммы двух зависимых величин
- Что такое bootstraping, почему это работает и недостатки

## Data Science

- Алгоритмы классификации и регрессии
- В каких случаях можно использовать линейные алгоритмы?
- Почему бутстрап-алгоритмы настолько хороши?
- Метрики качества
- Про AUC ROC. Преимущества, как строится, в чем смысл.
- Что такое бутстрапинг
- Почему ансамблирование работает? Bias-Variance tradoff
- Переобучается ли RandomForest с ростом количества деревьев?
- Как уменьшить корреляцию алгоритмов?
- Валидация, зачем и как
- Алгоритмы кластеризации
- Методы регуляризации (в нейронных сетях и обычных алгоритмов)
- Прикол L1 регуляризации и её объяснение
- Техники визуализации данных
- Техники генерации фичей
- Мешает ли линейным моделям корреляция признаков? Методы обучения линейных моделей. Проблемы точного решения и их решения
- Отличие mean и average
- Как строится ROC кривая. Что означают её оси
- Постановка задачи линейной регрессии и основные предположения. Функция потерь и методы оптимизации
- Логистическая регрессия
- Можем ли использовать ММП для регрессии и среднеквадратичную ошибку для логистической регрессии?
- Решающие деревья
- SVM. Kernel Trick
- Bayesian vs frequentist probability interpretations
- Методы обработки категориальных переменных
- EM Algorithm. Gaussian Mixture Model
- K-Means
- KNN
- PCA
- SVD
- ExtraTrees
- Hyper parameters search
- Time serires data. (S)ARIMA
- Фильтры с временными рядами
- Как выявить нелинейную зависимость между двумя выборками. Тест хи-квадрат и Колмогорова-Смирнова
- Почему модель может выдать результаты намного хуже на тестовом датасете, по сравнению с валидацией?

## Neurokeks

- Backpropogation. Chain rule
- Optimizers
- Activation functions
- Dataset augmentation
- Initialization
- Batcnorm layer. Why it works. Train and validation
- Dropout layer. Why it works. Train and validation
- If we will decrese input size of simple CNN network (without linear layers) by two times, how will change number of trainable params? What if there is linear layers?
- Task of language modeling
- Classic NLP tasks when using recurrent neural models
- Attention, Transformers
- BERT. What it accepts as input and what is its outputs
- CNN networks. Typical architecture
- Network finetuning for CNN networks
- Residual connections in CV models
- Can we apply Dropout layers for CNN networks?
- Нейросеть с двумя линейными "широкими" слоями способна аппроксимировать любую гладкую функцию. Почему мы используем вместо широких глубокие сети?
- Vanishing and exploding gradients